Proposal: Merge Template Generators and Vtables
===============================================

.. Note::
	Feature awaiting further design consideration.

This is a proposal to significantly reduce the overhead of templates by merging template generators and vtables.

Current issues
--------------

Size of typename
~~~~~~~~~~~~~~~~

``typename_t`` is currently equivalent to:

.. code-block:: c++

	struct typename_t {
		void* vtable;
		void* template_generator;
		uint64_t path;
	};

Likely overhead:

* 32-bit machine: size = 16 bytes, overhead versus pointer = 4x
* 64-bit machine: size = 24 bytes, overhead versus pointer = 3x

The size of ``typename_t`` is important because the template generators produce an array of typename, which must be pre-allocated on the stack. The aim is to reduce this to just a single pointer.

Size of ref
~~~~~~~~~~~

References to interfaces are currently equivalent to:

.. code-block:: c++

	struct ref_t {
		void* this;
		void* vtable;
		void* template_generator;
		uint64_t path;
	};

Likely overhead:

* 32-bit machine: size = 20 bytes, overhead versus pointer pair = 2.5x
* 64-bit machine: size = 32 bytes, overhead versus pointer pair = 2x

This means that every ``Interface&`` type is large. The aim is to reduce this to just a pair of the ``this`` pointer and the ``vtable`` pointer.

Multiple indirect calls
~~~~~~~~~~~~~~~~~~~~~~~

Consider:

.. code-block:: c++

	template <typename T>
	class Example {
		void method();
	}
	
	void f(Example& object) {
		g(object);
	}
	
	void g(Interface& object) {
		object.method();
	}

This code involves two indirect calls:

* The ``object.method()`` call in ``g()`` will call ``Example::method()``.
* ``Example::method()`` calls its template generator (which from its perspective is just a function pointer) to get the template arguments.

Approach
--------

Accessing vtable via template generator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Considering the interface reference:

.. code-block:: c++

	struct ref_t {
		void* this;
		void* vtable;
		void* template_generator;
		uint64_t path;
	};

We can make the following observations:

* The language allows any type, including primitives like ``int``, to be used polymorphically, so we can't get any type information into/out of the ``this`` pointer. We therefore need at least an extra pointer.
* The ``vtable`` is generated by the compiler at a point where the template arguments are **unknown**. For example, we know the vtable of ``std::varray<T>``, but type ``T`` may come from another module.
* There is a chain of template generators from the point where template arguments are known to where the vtable is known.

This means that the template generator could be used to call into the vtable, turning the interface reference type into:

.. code-block:: c++

	struct ref_t {
		void* this;
		void* vtable;
		uint64_t path;
	};

And ``typename_t`` would become:

.. code-block:: c++

	struct typename_t {
		void* vtable;
		uint64_t path;
	};

Calls through template generator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dynamic dispatch must then call the template generator. This effectively means template generators no longer produce template arguments, but instead they call into a target function/method and give it the arguments directly.

A root template generator might now look like:

.. code-block:: c++

	// 64 bits in a path.
	const size_t PATH_BITS = 64;
	
	struct callinfo_t {
		uint64_t method_hash;
		typename_t types[8];
		void* root_template_generator;
		uint64_t path;
		size_t path_position;
	};
	
	<return arg> ROOT_0(hidden callinfo_t* callinfo, ...<call args>...) {
		callinfo->root_template_generator = ROOT_0;
		callinfo->types[0] = { VTABLE_int, 0 };
		callinfo->path_position = PATH_BITS - 1 - ctlz(callinfo->path)
		return tailcall TPLGEN_g(callinfo, ...<call args>...);
	}

We would generate a vtable corresponding to the template generator like so:

.. code-block:: c++

	const vtable_t ROOT_0_vtable = { ROOT_0, ROOT_0, ... };

An intermediate template generator might look like:

.. code-block:: c++

	<return arg> TPLGEN_g(hidden callinfo_t* callinfo, ...<call args>...) {
		if (callinfo->position == 0) {
			// End of path => call g(); g() uses callinfo to get its
			// template arguments.
			return tailcall g(callinfo, ...<call args>...);
		}
		
		callinfo->position--;
		const auto subPath = (callinfo->path >> callinfo->position);
		const auto mask = 0x3;
		const auto component = (subPath & mask);
		
		switch (component) {
		case 0:
			// Template parameters for 'i<T, byte>()':
			//   * 'T': first argument of parent, so just copy it across.
			//   * 'byte': add vtable and null template generator.
			callinfo->types[1] = { VTABLE_byte, 0 };
			
			// Still going => pass types to generator for 'i()'.
			return tailcall TPLGEN_i(callinfo, ...<call args>...);
		case 1:
			// Template parameters for 'j<T, byte>()':
			//   * 'T': first argument of parent, so just copy it across.
			//   * 'short': add vtable and null template generator.
			callinfo->types[1] = { VTABLE_short, 0 };
			
			// Still going => pass types to generator for 'j()'.
			return tailcall TPLGEN_j(callinfo, ...<call args>...);
		default:
			unreachable;
		}
	}

Dynamic dispatch calls might look like:

.. code-block:: c++

	int callMethod(ref_t reference, int arg0, int arg1) {
		typedef int (*method_type)(hidden callinfo_t*, int, int);
		method_type ptr = reference.vtable[METHOD_HASH_INDEX];
		
		callinfo_t callinfo;
		
		// Must set this to disambiguate which method should be called.
		callinfo.method_hash = METHOD_HASH;
		
		// Must set this to disambiguate which route to take down the template generator graph.
		callinfo.path = reference.path;
		
		return ptr(&callinfo, arg0, arg1);
	}

By calling through the template generator we also eliminate the extra indirect call when the function/method tries to access its own template arguments.

Encoding path into vtable pointer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The solution described above reduces an interface reference down to:

.. code-block:: c++

	struct ref_t {
		void* this;
		void* vtable;
		uint64_t path;
	};

We would like to remove the ``path`` element; this can only be achieved by encoding the path into the vtable pointer.

For a start, there are some available bits due to the alignment of vtables. If a vtable contained 16 pointers then we would have:

* 32-bit machine: vtable size = 64 bytes, (set align=size), available bits = 6 bits
* 64-bit machine: vtable size = 128 bytes, (set align=size), available bits = 7 bits

Dynamic dispatch calls would then have to clear the bits when calling a method:

.. code-block:: c++

	int callMethod(ref_t reference, int arg0, int arg1) {
		typedef int (*method_type)(hidden callinfo_t*, int, int);
		
		void** fixed_vtable = reference.vtable & ~(127);
		method_type ptr = fixed_vtable[METHOD_HASH_INDEX];
		
		callinfo_t callinfo;
		
		// Must set this to disambiguate which method should be called.
		callinfo.method_hash = METHOD_HASH;
		
		// Must set this to disambiguate which route to take down the template generator graph.
		callinfo.path = reference.vtable & 127;
		
		return ptr(&callinfo, arg0, arg1);
	}

However this relies on the path fitting into the available bits.

Reduce callinfo size
~~~~~~~~~~~~~~~~~~~~

After encoding the path into the vtable pointer, we can reduce ``callinfo_t`` to:

.. code-block:: c++

	struct callinfo_t {
		uint64_t method_hash;
		typename_t types[8];
		void* vtable;
		size_t path_position;
	};

In this case the root template generator no longer needs to set ``callinfo->root_template_generator`` and the dynamic dispatch code sets ``callinfo->vtable``.

Furthermore, ``typename_t`` is now just a vtable pointer, so that has already significantly reduced the size of ``callinfo_t``.

Encoding larger paths into vtable pointer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A simple solution to encode larger paths into the vtable pointer is to produce a larger vtable for the root template generator. For example, a vtable could be repeated a power of two number of times in memory to get extra available bits:

* 2 contiguous copies of vtable: extra 1 bit
* 4 contiguous copies of vtable: extra 2 bits
* 8 contiguous copies of vtable: extra 3 bits
* etc.

Increasing the size of the allocated space for the vtable effectively means allocating more bits in the address. Regardless of the machine you have:

* 8 available bits: requires 256 bytes of vtable data
* 9 available bits: requires 512 bytes of vtable data
* 10 available bits: requires 1024 bytes of vtable data
* etc.

Clearly, the memory required is exponential in terms of the number of path bits required. So it's important to keep the path size as small as possible. Another issue is that the root template generator must know that it needs to allocated more vtable data, so that we can minimise space overhead in most cases.

Reducing path size using modules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Path bits are currently allocated for every templated function/method calls **within** a module, but this is unnecessary. We can perform template substitution within modules, so we only need to allocate path bits for calls from one module to another.

.. code-block:: c++

	// ---- Module 1.
	void a() {
		b<int>();
	}
	
	// ---- Module 2.
	template <typename T>
	void b() {
		c<T, float>();
	}
	
	template <typename S, typename T>
	void c() {
		d<T, S>();
	}
	
	// ---- Module 3.
	template <typename S, typename T>
	void h();

We can partially substitute ``c()`` to produce:

.. code-block:: c++

	// ---- Module 1.
	void a() {
		b<int>();
	}
	
	// ---- Module 2.
	template <typename T>
	void b() {
		c_SUBSTITUTED<T>();
	}
	
	template <typename T>
	void c_SUBSTITUTED() {
		d<float, T>();
	}
	
	// ---- Module 3.
	template <typename S, typename T>
	void d();

Effectively the substitutions pass the template arguments as received to our module directly around our module's code unmodified. For example:

.. code-block:: c++

	// All c
	template <typename T>
	export void f(T value) {
		g<wrapper<T>>(wrapper<T>(value));
	}
	
	template <typename T>
	void g(T value) {
		h<T>(value);
	}
	
	template <typename T>
	import void h(T value);

This becomes:

.. code-block:: c++

	template <typename T>
	void f(T value) {
		g_SUBSTITUTED<T>(wrapper<T>(value));
	}
	
	template <typename T>
	void g_SUBSTITUTED(wrapper<T> value) {
		h<wrapper<T>>(value);
	}
	
	template <typename T>
	import void h(T value);

These substitutions mean that all code in our module can use the same path value (for a given template generator graph).

Determining path size at compile-time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One possible approach is to add a ``depth`` attribute to imported templates, which indicates how many bits they require in their path:

.. code-block:: c++

	template(depth 2) <typename T>
	import void f(T value);

Not specifying the ``depth`` means that it is zero, and hence either:

* The module does not pass the template variables it is given to any other modules.
* The module passes template variables in the same form to other modules as it is given them, and those modules have ``depth=0``.

(The second case is the result of the pass-through optimisation.)

This has the following advantages:

* A known depth means root template generators know how many bits must be available and hence can allocate vtable sizes accordingly.
* The compiler can warn when the ``depth`` becomes large enough that the template generator vtable is huge (at 12+ bits it starts taking 4+KiB).
* We can prevent template cycles between modules, because they would end up with infinite depth.
* We can remove ``path_position`` from ``callinfo_t``, because each intermediate template generator knows exactly its offset within the path.

Determining path size at run-time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``depth`` attribute exposes an implementation detail of the module, so it would be preferable to avoid it. Hence an alternative is to compute the depth at run-time.

Doing this at run-time means root template generator vtables can't be pre-allocated. We can call down the chain at load-time to determine the depth, but we can't allocate storage in the data segment this way. We can allocate a one-vtable size global, but the required depth may exceed this space (i.e. when more than 6/7 bits are needed in the path). There are two approaches to this:

* Terminate/report error. With this approach we can omit the load-time call for non-debug builds.
* Allocate (suitably aligned) space on the heap (and copy from the vtable global).

Direct versus indirect calls
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An interesting observation is that the path used by template generators is effectively a re-creation of the stack. The only reason we can't use the stack is that we can capture references to templated functions, methods or classes:

.. code-block:: c++

	template <typename T>
	Interface& cast_to_interface(Class<T>& object) {
		return object;
	}

Any method of ``Class<T>`` that is called via ``Interface&`` is likely to want to access its template arguments, but by that point the stack frame of ``f()`` will have unwound, so we can't just put the template arguments on the stack.

**However**, in most cases we can put the template arguments on the stack:

.. code-block:: c++

	template <typename T>
	void f() {
		g<T>();
	}

In this case we don't actually need a template generator, because ``f()`` can pass its arguments to ``g()`` on the stack. An important caveat is that ``g()`` might capture references to templated functions, methods or classes, so we must pass it a template generator it can use.

Based on this reasoning, the path only needs to contain enough information to identify **indirectly called** templated functions, methods or classes. So the template generator for ``f()`` never needs to consider the possibility of an indirect call to ``g()``, because there is no way to achieve that in the code.

The template generator for ``f()`` does, however, need to consider the possibility that ``g()`` performs an indirect call to templated functions, so it still needs to call the template generator for ``g()``.

Using vtable slots to reduce path size
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following template generator graph:

::

	Module 1   |   Module 2   |  Module 3  |
	
	    ->   TPL(f)  -> a (slot: 2)
	                 ->     TPL(a)  -> x (slot: 5)
	                 -> b (slot: 3)
	                 ->     TPL(b)  -> y (slot: 6)
	                 -> c (slot: 4)
	                 ->     TPL(c)  -> z (slot: 6)

Here ``a``, ``b`` and ``c`` are templated functions or methods for which an indirect call reference is captured by ``f``. ``x`` is a templated function or method for which an indirect call reference is captured by ``a``, and similarly applies for ``y`` (captured by ``b``) and ``z`` (captured by ``c``).

The compiler for module 2 assumes there are 6 choices for the template generator of ``f``:

* Call ``a``
* Call ``b``
* Call ``c``
* Call template generator for ``a``
* Call template generator for ``b``
* Call template generator for ``c``

However the template generator of ``f`` also knows the 64-bit hash of the name of the function or method it will eventually call. We can therefore divide these based on the vtable slot they will fall into, which is shown in the graph. Looking at the whole graph, if you take into account the vtable slots there is only one conflict: between ``b`` and ``c`` when the slot index is ``6``. In other words the template generator for ``f`` can have logic such as:

.. code-block:: c++

	switch (path_value) {
	case 0:
		switch (slot) {
		case 2: ...call a...
		case 3: ...call b...
		case 4: ...call c...
		case 5: ...call a template generator...
		case 6: ...call b template generator...
		}
	case 1:
		assert(slot == 6);
		...call c template generator...
	}

This works because we know that we can only be calling ``y`` and ``z`` when the slot index is ``6``; if the slot index is anything else the path is unambiguous.

To compute this, we can define ``reachable(N)`` for node ``N`` where:

* If ``N`` is a templated function/method (i.e. a leaf in the graph), then ``reachable(N) = { P }``, where ``P`` is its vtable slot.
* If ``N`` is a template generator (i.e. **not** a leaf), then ``reachable(N) = union(child C of N) { reachable(C) }``.

This gives:

* ``reachable(a) = { 2 }``
* ``reachable(TPLGEN(a)) = reachable(x) = { 5 }``
* ``reachable(b) = { 3 }``
* ``reachable(TPLGEN(b)) = reachable(y) = { 6 }``
* ``reachable(c) = { 4 }``
* ``reachable(TPLGEN(c)) = reachable(z) = { 6 }``
* ``reachable(f) = reachable(a) | ... | reachable(TPL(a)) | ... = { 2, 3, 4, 5, 6 }``

You can see where there are conflicts by determining ``conflict(N)``, which is ``union(child A of N, child B of N, A != B) { reachable(A) & reachable(B) }``:

* ``conflict(TPLGEN(a)) = { }``
* ``conflict(TPLGEN(b)) = { }``
* ``conflict(TPLGEN(c)) = { }``
* ``conflict(TPLGEN(f)) = (reachable(a) & reachable(b)) | (reachable(a) & reachable(c)) | ... = { 6 }``

By representing ``reachable(N)`` as a bit field (one bit per vtable slot to indicate whether there is a reachable function/method for that slot) we can write efficient code to perform this computation, since intersection and union map neatly onto bitwise ``AND`` and bitwise ``OR``.

The template generator can effectively be made programmable by creating a slot action table:

.. code-block:: c++

	struct template_child_info_t {
		// The value we put into our position in the path to identify
		// this child.
		uint16_t path_id;
		
		// Reachability set of this child represented as bitfield.
		uint16_t reachable;
		
		// Pointer to child's template_info_t; only applies for template
		// generators.
		template_info_t* info;
	};
	
	struct slot_action_t {
		// The action to take for each path ID, for this slot.
		uint8_t id_to_child_map[NUM_CHILDREN];
	};
	
	struct template_info_t {
		// The offset within the path of our component.
		uint8_t offset;
		
		// The mask of the path to get our component.
		uint16_t mask;
		
		// The reachability of each path ID.
		uint16_t path_id_reachability[NUM_CHILDREN];
		
		// The action table for each slot.
		slot_action_t slot_actions[VTABLE_SIZE];
		
		// Information about child templates.
		template_child_info_t children[NUM_CHILDREN];
	};

The template generator for ``f`` would then look like:

.. code-block:: c++

	template_info_t f_info = ...;
	
	<return arg> TPLGEN_f(hidden callinfo_t* callinfo, ...<call args>...) {
		const auto slot = callinfo->method_hash & VTABLE_SIZE;
		const auto path_id = (callinfo->vtable >> f_info->offset) & f_info->mask;
		const auto action = f_info->slot_actions[slot].id_actions[path_id];
		
		switch (action) {
		case 0:
			[...modify types for templated function call...]
			return tailcall a(callinfo, ...<call args>...);
		case 1:
			[...modify types for templated function call...]
			return tailcall b(callinfo, ...<call args>...);
		case 2:
			[...modify types for templated function call...]
			return tailcall c(callinfo, ...<call args>...);
		case 3:
			[...modify types for templated function call...]
			return tailcall TPLGEN_a(callinfo, ...<call args>...);
		case 4:
			[...modify types for templated function call...]
			return tailcall TPLGEN_b(callinfo, ...<call args>...);
		case 5:
			[...modify types for templated function call...]
			return tailcall TPLGEN_c(callinfo, ...<call args>...);
		}
	}

The algorithm to generate the table would look something like:

.. code-block:: c++

	// Allocate a path ID for the given reachability set.
	uint8_t allocate_path_id(template_info_t* info, uint16_t reachable) {
		uint8_t id = 0;
		
		while (true) {
			if (info->path_id_reachability[id] & reachable) {
				// Conflicts with this reachability set, try
				// next.
				id++;
				continue;
			}
			
			// Union the reachability set given; this means the path
			// ID will only be re-used for other genuinely
			// non-conflicting reachability sets.
			info->path_id_reachability[id] |= reachable;
			
			return id;
		}
	}
	
	void generate_slotactions(template_info_t* info) {
		size_t max_offset = 0;
		
		// Recursive call to children.
		for (size_t i = 0; i < NUM_CHILDREN; i++) {
			template_info_t* child_info = info->children[i].info;
			if (child_info == NULL) continue;
			
			const auto offset = generate_slotactions(child_info);
			if (offset > max_offset) max_offset = offset;
		}
		
		size_t max_path_id = 0;
		
		// Allocate a path ID for each child based on their reachability sets.
		for (size_t i = 0; i < NUM_CHILDREN; i++) {
			const auto path_id = allocate_path_id(info, child_info->reachable);
			
			// Set the path ID we'll use for this child.
			info->children[i].path_id = path_id;
			
			if (path_id > max_path_id) max_path_id = path_id;
		}
		
		// Determine offset and mask.
		info->offset = max_offset;
		info->mask = round_up_to_power_of_2(max_path_id);
		
		// Fill in the slot action tables.
		for (unsigned slot = 0; slot < VTABLE_SIZE; slot++) {
			for (size_t i = 0; i < NUM_CHILDREN; i++) {
				if (!(info->children[j].reachable & (1 << slot))) {
					// Child isn't reachable for this slot.
					continue;
				}
				
				const auto child_path_id = info->children[i].path_id;
				info->slot_actions[slot].id_to_child_map[child_path_id] = i;
			}
		}
		
		return max_offset + log_2(info->mask);
	}

.. Note::
	We can go even further than this and use the complete 64-bit name hashes, which in general shouldn't conflict unless the names are the same. This would require determining if there are any identical name hashes between children (only in such cases do we need to allocate bits on the path). However doing this is costly, so the approximation of using only 4-bits in the hash value (corresponding to 16 possible slots) is more useful.

Summary
-------

This section describes the result of implementing all the ideas in the proposal.

callinfo_t
~~~~~~~~~~

.. code-block:: c++

	struct callinfo_t {
		uint64_t method_hash;
		void* vtable;
		typename_t types[8];
	};

Interface reference
~~~~~~~~~~~~~~~~~~~

.. code-block:: c++

	struct ref_t {
		void* this;
		void* vtable;
	};
	
	<return arg> call(ref_t ref, ...<call args>...) {
		typedef <return arg> (*method_type)(hidden callinfo_t*, ...);
		
		void** fixed_vtable = ref.vtable & ~(127);
		method_type ptr = fixed_vtable[METHOD_HASH % VTABLE_SIZE];
		
		callinfo_t callinfo;
		callinfo.method_hash = METHOD_HASH;
		callinfo.vtable = ref.vtable;
		return ptr(&callinfo, ref.this, ...<call args>...);
	}

typename_t
~~~~~~~~~~

.. code-block:: c++

	struct typename_t {
		void* vtable;
	};
	
	<return arg> call(typename_t ref, ...<call args>...) {
		typedef <return arg> (*static_method_type)(hidden callinfo_t*, ...);
		
		void** fixed_vtable = ref.vtable & ~(127);
		static_method_type ptr = fixed_vtable[METHOD_HASH % VTABLE_SIZE];
		
		callinfo_t callinfo;
		callinfo.method_hash = METHOD_HASH;
		callinfo.vtable = ref.vtable;
		return ptr(&callinfo, ...<call args>...);
	}

Root template generator
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: c++
	
	<return arg> ROOT_0(hidden callinfo_t* callinfo, ...<call args>...) {
		callinfo->types[0] = { VTABLE_int };
		return tailcall INTERMEDIATE_0(callinfo, ...<call args>...);
	}

Root template generator vtable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A root template generator allocates space for one or more vtables, each of which are identical. The root template generator vtable will have an entry for each offset:

.. code-block:: c++

	const vtable_t ROOT_vtable = { ROOT_0, ROOT_1, ROOT_2, ROOT_3, ROOT_4, ... };

Intermediate template generator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: c++

	<return arg> INTERMEDIATE_0(hidden callinfo_t* callinfo, ...<call args>...) {
		const auto subPath = (callinfo->vtable >> NEXT_MODULE_TEMPLATE_DEPTH);
		const auto component = (subPath & 0x3);
		
		switch (component) {
		case 0:
			// Path terminated; call our own function.
			return tailcall our_exported_function(callinfo, ...<call args>...);
		case 1:
			// Pass the types to next intermediate generator.
			callinfo->types[1] = { VTABLE_byte };
			return tailcall NEXT_INTERMEDIATE_0(callinfo, ...<call args>...);
		case 2:
			// Pass the types to next intermediate generator.
			callinfo->types[1] = { VTABLE_short };
			return tailcall NEXT_INTERMEDIATE_0(callinfo, ...<call args>...);
		default:
			unreachable;
		}
	}

Intermediate template generator vtable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Intermediate template generators don't have a vtable, but they do have functions for each of the vtable offsets:

.. code-block:: c++

	<return arg> INTERMEDIATE_0(hidden callinfo_t* callinfo, ...<call args>...);
	<return arg> INTERMEDIATE_1(hidden callinfo_t* callinfo, ...<call args>...);
	<return arg> INTERMEDIATE_2(hidden callinfo_t* callinfo, ...<call args>...);
	// etc.

Receiving template arguments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: c++

	<return arg> exported_templated_function(hidden callinfo_t* callinfo, ...<call args>...) {
		typename_t first_arg = callinfo->types[0];
		typename_t second_arg = callinfo->types[1];
		
		return internal_templated_function(callinfo, ...<call args>...);
	}

Sending template arguments
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: c++

	<return arg> function(hidden callinfo_t* callinfo, ...<call args>...) {
		callinfo->vtable |= (TEMPLATE_PATH_VALUE << NEXT_MODULE_TEMPLATE_DEPTH);
		return imported_templated_function(callinfo, ...<call args>...);
	}

Conflict resolution stub
~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: c++

	<return arg> conflict_resolution_stub(hidden callinfo_t* callinfo, ...<call args>...) {
		switch (callinfo->method_hash) {
			case HASH_METHOD_0:
				return tailcall method0(callinfo, ...<call args>...);
			case HASH_METHOD_1:
				return tailcall method1(callinfo, ...<call args>...);
			default:
				unreachable;
		}
	}
