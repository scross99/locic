import double exp(double x) noexcept;

namespace ANN {
	
	//Sigmoid function (S shape)
	double sigmoid(double netinput) {
		return (1.0 / (1.0 + exp(-netinput)));
	}
	
	class Neuron(size_t numInputs, double activation, double gradient,
	             double learnRate, std::varray<double> weights,
	             std::varray<double> inputs, std::varray<double>[2] elig) {
		
		static create(size_t numInputs) {
			// We have an extra input that's a negative constant.
			auto neuronNumInputs = numInputs + 1u;
			
			auto weights = std::varray<double>();
			
			for (auto i: range<size_t>(0, neuronNumInputs)) {
				weights.push_back(0.1 * Random());
			}
			
			auto elig = { std::varray<double>(),
			              std::varray<double>() };
			
			for (auto i: range<size_t>(0, 2)) {
				auto& eligElement = elig[i];
				
				for (auto j: range<size_t>(0, neuronNumInputs)) {
					eligElement.push_back(0.0);
				}
			}
			
			auto activation = 0.0;
			auto gradient = 0.0;
			auto learnRate = 0.1 / numInputs.cast<double>();
			
			return @(neuronNumInputs, activation, gradient,
			         learnRate, move weights, std::varray<double>(),
			         move elig);
		}
		
		void getWeights(std::varray<double>& weights) {
			for (auto i: range<size_t>(0, @numInputs)) {
				weights.push_back(@weights[i]);
			}
		}
		
		void setWeights(size_t* pos, std::varray<double>& weights) {
			for (auto i: range<size_t>(0, @numInputs)) {
				@weights[i] = weights[*pos];
				*pos++;
			}
		}
		
		size_t getNumWeights() const {
			return @numInputs;
		}
		
		// Calculate result from vector of inputs.
		double update(const std::varray<double>& input) {
			@inputs = input.copy();
			
			double result = 0.0f;
			
			@inputs.resize(@numInputs, 0.0);
			
			@inputs[@numInputs - 1u] = -1.0;
			
			for (auto i: range<size_t>(0, @numInputs)) {
				result += @inputs[i] * @weights[i];
			}
			
			@activation = sigmoid(result);
			return @activation;
		}
		
		double getGradient(size_t i) const {
			return @gradient * @weights[i];
		}
		
		void zeroGradient(size_t index) {
			auto& elig = @elig[index];
			
			for (auto i: range<size_t>(0, @numInputs)) {
				elig[i] = 0.0;
			}
		}
		
		void gradient(size_t index, double gradient) {
			@gradient = gradient * @activation * (1.0 - @activation);
			
			auto& elig = @elig[index];
			
			for (auto i: range<size_t>(0, @numInputs)) {
				elig[i] = 1.0 * elig[i] + @gradient * @inputs[i];
			}
		}
		
		void error(size_t index, double error) {
			auto& elig = @elig[index];
			
			for (auto i: range<size_t>(0, @numInputs - 1u)) {
				@weights[i] += @learnRate * error * elig[i];
			}
		}
		
	}
	
}
